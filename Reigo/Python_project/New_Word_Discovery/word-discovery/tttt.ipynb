{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "\n",
    "import struct\n",
    "import os\n",
    "import six\n",
    "import codecs\n",
    "import math\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=u'%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Progress:\n",
    "    \"\"\"显示进度，自己简单封装，比tqdm更可控一些\n",
    "    iterator: 可迭代的对象；\n",
    "    period: 显示进度的周期；\n",
    "    steps: iterator可迭代的总步数，相当于len(iterator)\n",
    "    \"\"\"\n",
    "    def __init__(self, iterator, period=1, steps=None, desc=None):\n",
    "        self.iterator = iterator\n",
    "        self.period = period\n",
    "        if hasattr(iterator, '__len__'):\n",
    "            self.steps = len(iterator)\n",
    "        else:\n",
    "            self.steps = steps\n",
    "        self.desc = desc\n",
    "        if self.steps:\n",
    "            self._format_ = u'%s/%s passed' %('%s', self.steps)\n",
    "        else:\n",
    "            self._format_ = u'%s passed'\n",
    "        if self.desc:\n",
    "            self._format_ = self.desc + ' - ' + self._format_\n",
    "        self.logger = logging.getLogger()\n",
    "    def __iter__(self):\n",
    "        for i, j in enumerate(self.iterator):\n",
    "            if (i + 1) % self.period == 0:\n",
    "                self.logger.info(self._format_ % (i+1))\n",
    "            yield j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KenlmNgrams:\n",
    "    \"\"\"加载Kenlm的ngram统计结果\n",
    "    vocab_file: Kenlm统计出来的词(字)表；\n",
    "    ngram_file: Kenlm统计出来的ngram表；\n",
    "    order: 统计ngram时设置的n，必须跟ngram_file对应；\n",
    "    min_count: 自行设置的截断频数。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file, ngram_file, order, min_count):\n",
    "        self.vocab_file = vocab_file\n",
    "        self.ngram_file = ngram_file\n",
    "        self.order = order\n",
    "        self.min_count = min_count\n",
    "        self.read_chars()\n",
    "        self.read_ngrams()\n",
    "    def read_chars(self):\n",
    "        f = open(self.vocab_file)\n",
    "        chars = f.read()\n",
    "        f.close()\n",
    "        chars = chars.split('\\x00')\n",
    "        self.chars = [i.decode('utf-8') if six.PY2 else i for i in chars]\n",
    "    def read_ngrams(self):\n",
    "        \"\"\"读取思路参考https://github.com/kpu/kenlm/issues/201\n",
    "        \"\"\"\n",
    "        self.ngrams = [{} for _ in range(self.order)]\n",
    "        self.total = 0\n",
    "        size_per_item = self.order * 4 + 8\n",
    "        def ngrams():\n",
    "            with open(self.ngram_file, 'rb') as f:\n",
    "                while True:\n",
    "                    s = f.read(size_per_item)\n",
    "                    if len(s) == size_per_item:\n",
    "                        n = self.unpack('l', s[-8:])\n",
    "                        yield s, n\n",
    "                    else:\n",
    "                        break\n",
    "        for s, n in Progress(ngrams(), 100000, desc=u'loading ngrams'):\n",
    "            if n >= self.min_count:\n",
    "                self.total += n\n",
    "                c = [self.unpack('i', s[j*4: (j+1)*4]) for j in range(self.order)]\n",
    "                c = ''.join([self.chars[j] for j in c if j > 2])\n",
    "                for j in range(len(c)):\n",
    "                    self.ngrams[j][c[:j+1]] = self.ngrams[j].get(c[:j+1], 0) + n\n",
    "    def unpack(self, t, s):\n",
    "        return struct.unpack(t, s)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleTrie:\n",
    "    \"\"\"通过Trie树结构，来搜索ngrams组成的连续片段\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dic = {}\n",
    "        self.end = True\n",
    "    def add_word(self, word):\n",
    "        _ = self.dic\n",
    "        for c in word:\n",
    "            if c not in _:\n",
    "                _[c] = {}\n",
    "            _ = _[c]\n",
    "        _[self.end] = word\n",
    "    def tokenize(self, sent): # 通过最长联接的方式来对句子进行分词\n",
    "        result = []\n",
    "        start, end = 0, 1\n",
    "        for i, c1 in enumerate(sent):\n",
    "            _ = self.dic\n",
    "            if i == end:\n",
    "                result.append(sent[start: end])\n",
    "                start, end = i, i+1\n",
    "            for j, c2 in enumerate(sent[i:]):\n",
    "                if c2 in _:\n",
    "                    _ = _[c2]\n",
    "                    if self.end in _:\n",
    "                        if i + j + 1 > end:\n",
    "                            end = i + j + 1\n",
    "                else:\n",
    "                    break\n",
    "        result.append(sent[start: end])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_vocab(candidates, ngrams, order):\n",
    "    \"\"\"通过与ngrams对比，排除可能出来的不牢固的词汇(回溯)\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i, j in candidates.items():\n",
    "        if len(i) < 3:\n",
    "            result[i] = j\n",
    "        elif len(i) <= order and i in ngrams:\n",
    "            result[i] = j\n",
    "        elif len(i) > order:\n",
    "            flag = True\n",
    "            for k in range(len(i) + 1 - order):\n",
    "                if i[k: k+order] not in ngrams:\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                result[i] = j\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_corpus(texts, filename):\n",
    "    \"\"\"将语料写到文件中，词与词(字与字)之间用空格隔开\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, 'w', encoding='utf-8') as f:\n",
    "        for s in Progress(texts, 10000, desc=u'exporting corpus'):\n",
    "            s = ' '.join(s) + '\\n'\n",
    "            f.write(s)\n",
    "\n",
    "\n",
    "def count_ngrams(corpus_file, order, vocab_file, ngram_file, memory=0.5):\n",
    "    \"\"\"通过os.system调用Kenlm的count_ngrams来统计频数\n",
    "    其中，memory是占用内存比例，理论上不能超过可用内存比例。\n",
    "    \"\"\"\n",
    "    done = os.system(\n",
    "        './count_ngrams -o %s --memory=%d%% --write_vocab_list %s <%s >%s'\n",
    "        % (order, memory * 100, vocab_file, corpus_file, ngram_file)\n",
    "    )\n",
    "    if done != 0:\n",
    "        raise ValueError('Failed to count ngrams by KenLM.')\n",
    "\n",
    "\n",
    "def filter_ngrams(ngrams, total, min_pmi=1):\n",
    "    \"\"\"通过互信息过滤ngrams，只保留“结实”的ngram。\n",
    "    \"\"\"\n",
    "    order = len(ngrams)\n",
    "    if hasattr(min_pmi, '__iter__'):\n",
    "        min_pmi = list(min_pmi)\n",
    "    else:\n",
    "        min_pmi = [min_pmi] * order\n",
    "    output_ngrams = set()\n",
    "    total = float(total)\n",
    "    for i in range(order-1, 0, -1):\n",
    "        for w, v in ngrams[i].items():\n",
    "            pmi = min([\n",
    "                total * v / (ngrams[j].get(w[:j+1], total) * ngrams[i-j-1].get(w[j+1:], total))\n",
    "                for j in range(i)\n",
    "            ])\n",
    "            if math.log(pmi) >= min_pmi[i]:\n",
    "                output_ngrams.add(w)\n",
    "    return output_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 算法构建完毕，下面开始执行完整的构建词库流程 =======\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# 语料生成器，并且初步预处理语料\n",
    "# 这个生成器例子的具体含义不重要，只需要知道它就是逐句地把文本yield出来就行了\n",
    "def text_generator():\n",
    "    txts = glob.glob('corpus/short1/*.txt')\n",
    "    for txt in txts:\n",
    "        s = codecs.open(txt, encoding='utf-8').read()\n",
    "        s = s.replace(u'\\u3000', ' ').strip()\n",
    "        s = re.sub('\\s','',s) #把空格转为空\n",
    "        yield re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', s)\n",
    "\n",
    "\n",
    "min_count = 32\n",
    "order = 4\n",
    "corpus_file = 'thucnews.corpus.txt' # 语料保存的文件名\n",
    "vocab_file = 'thucnews.chars' # 字符集保存的文件名\n",
    "ngram_file = 'thucnews.ngrams' # ngram集保存的文件名\n",
    "output_file = 'thucnews.vocab' # 最后导出的词表文件名\n",
    "memory = 0.5 # memory是占用内存比例，理论上不能超过可用内存比例\n",
    "\n",
    "write_corpus(text_generator(), corpus_file) # 将语料转存为文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to count ngrams by KenLM.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12576/2525668188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 用Kenlm统计ngram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12576/3206192295.py\u001b[0m in \u001b[0;36mcount_ngrams\u001b[1;34m(corpus_file, order, vocab_file, ngram_file, memory)\u001b[0m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to count ngrams by KenLM.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to count ngrams by KenLM."
     ]
    }
   ],
   "source": [
    "count_ngrams(corpus_file, order, vocab_file, ngram_file, memory) # 用Kenlm统计ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngrams = KenlmNgrams(vocab_file, ngram_file, order, min_count) # 加载ngram\n",
    "ngrams = filter_ngrams(ngrams.ngrams, ngrams.total, [0, 2, 4, 6]) # 过滤ngram\n",
    "ngtrie = SimpleTrie() # 构建ngram的Trie树\n",
    "\n",
    "for w in Progress(ngrams, 100000, desc=u'build ngram trie'):\n",
    "    _ = ngtrie.add_word(w)\n",
    "\n",
    "candidates = {} # 得到候选词\n",
    "for t in Progress(text_generator(), 1000, desc='discovering words'):\n",
    "    for w in ngtrie.tokenize(t): # 预分词\n",
    "        candidates[w] = candidates.get(w, 0) + 1\n",
    "\n",
    "# 频数过滤\n",
    "candidates = {i: j for i, j in candidates.items() if j >= min_count}\n",
    "# 互信息过滤(回溯)\n",
    "candidates = filter_vocab(candidates, ngrams, order)\n",
    "\n",
    "# 输出结果文件\n",
    "with codecs.open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for i, j in sorted(candidates.items(), key=lambda s: -s[1]):\n",
    "        s = '%s %s\\n' % (i, j)\n",
    "        f.write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "046e79a65ec89518396a274026599ddf48e2ffaf1fe6744c6fe1de54e9d05a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
